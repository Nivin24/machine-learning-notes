{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (KNN): An End-to-End Walkthrough\n",
    "\n",
    "This notebook provides a self-contained, educational walkthrough of KNN for classification using scikit-learn. We'll cover:\n",
    "1) Intro to KNN, 2) Load the Iris dataset, 3) Explore and scale features, 4) Fit baseline KNN, 5) Try different k and distance metrics, 6) Visualize decision boundaries in 2D, 7) Evaluate accuracy and plot error vs. k, 8) Explanations throughout, 9) Cross-validation to find optimal k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) What is KNN?\n",
    "KNN is a non-parametric, instance-based learning algorithm. For a new point, it finds the k closest training samples (by a chosen distance metric) and predicts by majority vote (classification) or average (regression).\n",
    "- Key choices: number of neighbors k, distance metric (Euclidean/Minkowski/Manhattan/Chebyshev), weighting (uniform vs. distance).\n",
    "- Sensitivities: feature scaling, noisy data, high-dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load libraries and dataset (Iris)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='target')\n",
    "class_names = iris.target_names\n",
    "X.head(), y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Exploration and scaling\n",
    "We quickly inspect distributions and pairwise relations. KNN is distance-based, so scaling features is important to avoid dominance by large-scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic stats and visualization\n",
    "display(X.describe())\n",
    "sns.pairplot(pd.concat([X, y.map(dict(enumerate(class_names))).rename('class')], axis=1),\n",
    "             hue='class', corner=True)\n",
    "plt.suptitle('Iris Pairplot by Class', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# Scale features (fit on train, transform train and test)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Baseline KNN with default k\n",
    "We'll start with k=5 (sklearn default), uniform weights, Euclidean distance (Minkowski with p=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='minkowski', p=2)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'Baseline test accuracy (k=5, Euclidean): {acc:.3f}')\n",
    "print('\nClassification report:\n', classification_report(y_test, y_pred, target_names=class_names))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Baseline KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Test various distance metrics and k values\n",
    "We compare Euclidean (p=2), Manhattan (p=1), and Chebyshev (Lâˆž) distances across k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(1, 31))\n",
    "metrics = [('minkowski', 2, 'Euclidean'), ('minkowski', 1, 'Manhattan'), ('chebyshev', None, 'Chebyshev')]\n",
    "results = {name: [] for _, _, name in metrics}\n",
    "for metric, p, name in metrics:\n",
    "    for k in ks:\n",
    "        clf = KNeighborsClassifier(n_neighbors=k, metric=metric, **({'p': p} if p is not None else {}))\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        pred = clf.predict(X_test_scaled)\n",
    "        results[name].append(accuracy_score(y_test, pred))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for name, accs in results.items():\n",
    "    plt.plot(ks, accs, label=name)\n",
    "plt.xlabel('k (number of neighbors)')\n",
    "plt.ylabel('Accuracy on test set')\n",
    "plt.title('Accuracy vs. k for different distance metrics')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Decision boundary visualization (2D projection)\n",
    "We project to two selected features to visualize KNN decision boundaries. Note that this trades off realism for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose two features, e.g., petal length and petal width (often most separable)\n",
    "feat_x, feat_y = 'petal length (cm)', 'petal width (cm)'\n",
    "X2 = X[[feat_x, feat_y]]\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.25, stratify=y, random_state=42)\n",
    "sc2 = StandardScaler()\n",
    "X2_train_s = sc2.fit_transform(X2_train)\n",
    "X2_test_s = sc2.transform(X2_test)\n",
    "\n",
    "# Fit a simple KNN (k=5) on these two features\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2.fit(X2_train_s, y2_train)\n",
    "\n",
    "# Create a mesh grid\n",
    "h = 0.02\n",
    "x_min, x_max = X2_train_s[:,0].min() - 1, X2_train_s[:,0].max() + 1\n",
    "y_min, y_max = X2_train_s[:,1].min() - 1, X2_train_s[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = knn2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='Pastel1')\n",
    "sns.scatterplot(x=X2_train_s[:,0], y=X2_train_s[:,1], hue=y2_train.map(dict(enumerate(class_names))), s=40, edgecolor='k')\n",
    "plt.xlabel(feat_x + ' (scaled)')\n",
    "plt.ylabel(feat_y + ' (scaled)')\n",
    "plt.title('KNN Decision Boundaries (k=5) on Two Features')\n",
    "plt.legend(title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Error rate vs. k\n",
    "We compute error rate (1 - accuracy) for a range of k and plot it to visualize the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for k in ks:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    pred = clf.predict(X_test_scaled)\n",
    "    errors.append(1 - accuracy_score(y_test, pred))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(ks, errors, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Error rate (1 - accuracy)')\n",
    "plt.title('KNN Error Rate vs. k')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Notes and guidance\n",
    "- Small k => low bias, high variance; large k => higher bias, lower variance.\n",
    "- Always scale features for KNN.\n",
    "- Try distance-weighted voting (weights='distance') when class overlap exists.\n",
    "- Tune k and metric via cross-validation rather than the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Cross-validation to find optimal k and metric\n",
    "We run GridSearchCV over k, metric, and weights with 5-fold CV using the scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski', 'chebyshev'],\n",
    "    'p': [1, 2]  # only used when metric='minkowski'\n",
    "}\n",
    "\n",
    "# Custom scorer (default is accuracy)\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best CV accuracy:', grid.best_score_)\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "y_pred_best = best_knn.predict(X_test_scaled)\n",
    "print('Test accuracy with best model:', accuracy_score(y_test, y_pred_best))\n",
    "print('\nClassification report (best model):\n', classification_report(y_test, y_pred_best, target_names=class_names))\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Greens', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Best KNN (CV)')\n",
    "plt.show()\n",
    "\n",
    "# Plot CV mean scores vs k for Euclidean and Manhattan (for illustration)\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "subset = cv_results[(cv_results['param_metric']=='minkowski') & (cv_results['param_weights']=='uniform')]\n",
    "plt.figure(figsize=(8,5))\n",
    "for p, label in [(1, 'Manhattan (p=1)'), (2, 'Euclidean (p=2)')]:\n",
    "    tmp = subset[subset['param_p']==p].sort_values('param_n_neighbors')\n",
    "    plt.plot(tmp['param_n_neighbors'], tmp['mean_test_score'], marker='o', label=label)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('CV accuracy')\n",
    "plt.title('CV accuracy vs. k (uniform weights)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "- Scaling is crucial.\n",
    "- Optimal k balances bias and variance and depends on the dataset.\n",
    "- Consider metric and weights; cross-validation provides an unbiased model selection framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3", 
   "language": "python", 
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {"name": "ipython", "version": 3},
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
