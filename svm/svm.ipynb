{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM) Notebook\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Linear vs kernel SVM on synthetic data\n",
    "- Margin visualization\n",
    "- Hyperparameter tuning (C, gamma)\n",
    "- Kernel comparison (linear, polynomial, RBF)\n",
    "\n",
    "See accompanying markdown files in this repo for theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Linear separable-ish synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=600, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=1, class_sep=1.5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lin_clf = Pipeline([('scaler', StandardScaler()), ('svc', LinearSVC(C=1.0, random_state=42))])\n",
    "lin_clf.fit(X_train, y_train)\n",
    "y_pred = lin_clf.predict(X_test)\n",
    "print('LinearSVC accuracy:', accuracy_score(y_test, y_pred))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin visualization (linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract linear decision boundary and margins from LinearSVC approximation via SVC linear kernel\n",
    "lin_svc = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='linear', C=1.0, random_state=42))])\n",
    "lin_svc.fit(X_train, y_train)\n",
    "svc = lin_svc.named_steps['svc']\n",
    "w = svc.coef_[0]\n",
    "b = svc.intercept_[0]\n",
    "\n",
    "def plot_decision_boundary_and_margin(X, y, clf, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    # grid\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.coolwarm)\n",
    "    # data\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.coolwarm, edgecolor='k')\n",
    "    # decision boundary and margins in original space require inverse scaling\n",
    "    # compute line: w.x + b = 0 and margins: = Â±1\n",
    "    # line in scaled space; we plot via decision function contours\n",
    "    ax = plt.gca()\n",
    "    # Plot decision function contours\n",
    "    Zf = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    cs = ax.contour(xx, yy, Zf, colors=['k','k','k'], levels=[-1,0,1], linestyles=['--','-','--'])\n",
    "    ax.clabel(cs, inline=True, fontsize=8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x1'); plt.ylabel('x2'); plt.tight_layout()\n",
    "\n",
    "plot_decision_boundary_and_margin(X_test, y_test, lin_svc, 'Linear SVM: boundary and margins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Nonlinear datasets: moons and circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_kernel_svm(make_data_fn, title, param_grid=None):\n",
    "    X, y = make_data_fn(noise=0.2, random_state=42, n_samples=600) if make_data_fn!=make_circles else make_data_fn(noise=0.1, factor=0.5, random_state=42, n_samples=600)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'svc__kernel': ['rbf'],\n",
    "            'svc__C': [0.1, 1, 10],\n",
    "            'svc__gamma': ['scale', 0.1, 1]\n",
    "        }\n",
    "    gs = GridSearchCV(pipe, param_grid, cv=3, n_jobs=-1)\n",
    "    gs.fit(X_tr, y_tr)\n",
    "    print(title, 'best params:', gs.best_params_)\n",
    "    best = gs.best_estimator_\n",
    "    y_pred = best.predict(X_te)\n",
    "    print('Accuracy:', accuracy_score(y_te, y_pred))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_te, y_pred); plt.show()\n",
    "    # plot decision regions\n",
    "    x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
    "    y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "    Z = best.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.coolwarm)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=12)\n",
    "    plt.title(title + ' decision regions')\n",
    "    plt.xlabel('x1'); plt.ylabel('x2'); plt.tight_layout(); plt.show()\n",
    "\n",
    "train_plot_kernel_svm(make_moons, 'Moons (RBF SVM)')\n",
    "train_plot_kernel_svm(make_circles, 'Circles (RBF SVM)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Kernel comparison on moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=600, noise=0.2, random_state=42)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "kernels = [\n",
    "    ('linear', {'svc__kernel':['linear'], 'svc__C':[0.1,1,10]}),\n",
    "    ('poly',   {'svc__kernel':['poly'], 'svc__degree':[2,3], 'svc__C':[0.1,1,10], 'svc__gamma':['scale', 0.1], 'svc__coef0':[0,1]}),\n",
    "    ('rbf',    {'svc__kernel':['rbf'], 'svc__C':[0.1,1,10], 'svc__gamma':['scale', 0.1, 1]})\n",
    "]\n",
    "results = {}\n",
    "for name, grid in kernels:\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "    gs = GridSearchCV(pipe, grid, cv=3, n_jobs=-1)\n",
    "    gs.fit(X_tr, y_tr)\n",
    "    best = gs.best_estimator_\n",
    "    acc = best.score(X_te, y_te)\n",
    "    results[name] = (acc, gs.best_params_)\n",
    "\n",
    "for k, (acc, params) in results.items():\n",
    "    print(f'{k}: acc={acc:.3f}, params={params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Notes\n",
    "- Scale inputs for kernel SVMs\n",
    "- Use cross-validation to tune C/gamma/degree\n",
    "- For very large datasets, consider LinearSVC or approximate kernels\n",
    "- Inspect number of support vectors: best_estimator_.named_steps['svc'].n_support_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
